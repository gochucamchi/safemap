# -*- coding: utf-8 -*-
"""
ì‹¤ì¢…ì ì‚¬ì§„ ìŠ¤í¬ë© ì„œë¹„ìŠ¤
- Rate limiting ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´ ë° ì¬ì‹œë„ ë¡œì§
- ì§„í–‰ ìƒí™© ì €ì¥ ë° ì¬ê°œ
- MD5 í•´ì‹œë¥¼ í†µí•œ ì¤‘ë³µ ì‚¬ì§„ í•„í„°ë§
"""

import asyncio
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
import httpx


class PhotoScraperService:
    """ì‹¤ì¢…ì ì‚¬ì§„ ìŠ¤í¬ë© ì„œë¹„ìŠ¤"""

    # í”Œë ˆì´ìŠ¤í™€ë” ì´ë¯¸ì§€ í¬ê¸° (ê±´ë„ˆë›°ê¸°)
    PLACEHOLDER_SIZE = 2860

    def __init__(self, delay: float = 3.0, max_retries: int = 3):
        """
        Args:
            delay: ìš”ì²­ ê°„ ê¸°ë³¸ ë”œë ˆì´ (ì´ˆ)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        """
        self.delay = delay
        self.max_retries = max_retries
        self.session: Optional[httpx.AsyncClient] = None

    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.session = httpx.AsyncClient(
            timeout=30.0,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0",
                "Referer": "https://www.safe182.go.kr/",
                "Origin": "https://www.safe182.go.kr"
            },
            follow_redirects=True
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.aclose()

    def _get_md5(self, data: bytes) -> str:
        """ë°ì´í„°ì˜ MD5 í•´ì‹œ ê³„ì‚°"""
        return hashlib.md5(data).hexdigest()

    async def _download_with_retry(self, url: str, retry_count: int = 0) -> Optional[httpx.Response]:
        """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ë‹¤ìš´ë¡œë“œ"""
        try:
            response = await self.session.get(url)
            response.raise_for_status()
            return response

        except (httpx.HTTPError, httpx.RemoteProtocolError) as e:
            if retry_count < self.max_retries:
                # Exponential backoff: 2ì´ˆ, 4ì´ˆ, 8ì´ˆ
                wait_time = 2 ** (retry_count + 1)
                print(f"  âš ï¸  ì˜¤ë¥˜ ë°œìƒ: {str(e)[:50]}... {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ({retry_count + 1}/{self.max_retries})")
                await asyncio.sleep(wait_time)
                return await self._download_with_retry(url, retry_count + 1)
            else:
                print(f"  âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {str(e)[:100]}")
                return None

    async def scrape_person_photos(self, external_id: str, name: str = "") -> List[str]:
        """
        íŠ¹ì • ì‹¤ì¢…ìì˜ ì‚¬ì§„ URL ìŠ¤í¬ë©

        Args:
            external_id: ì‹¤ì¢…ì ID (msspsnIdntfccd)
            name: ì‹¤ì¢…ì ì´ë¦„ (ë¡œê¹…ìš©)

        Returns:
            ì‚¬ì§„ URL ë¦¬ìŠ¤íŠ¸
        """
        # 1. ìƒì„¸ í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸ (ì„¸ì…˜ ìƒì„±)
        detail_url = f"https://www.safe182.go.kr/home/lcm/lcmMssGet.do?msspsnIdntfccd={external_id}&rptDscd=2"

        print(f"\n{'='*80}")
        print(f"ğŸ“¸ ì‚¬ì§„ ìŠ¤í¬ë©: {name} (ID: {external_id})")
        print(f"{'='*80}")

        # ìƒì„¸ í˜ì´ì§€ ì ‘ì†
        response = await self._download_with_retry(detail_url)
        if not response:
            print("  âŒ ìƒì„¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨")
            return []

        print("  âœ… ìƒì„¸ í˜ì´ì§€ ì ‘ì† ì„±ê³µ")

        # 2. ì„¸ì…˜ ìœ ì§€í•˜ë©´ì„œ ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ
        photo_urls = []
        seen_hashes = set()

        # ìµœëŒ€ 10ê°œê¹Œì§€ ì‹œë„ (ì¸ë±ìŠ¤ ê¸°ë°˜)
        for idx in range(10):
            photo_url = f"https://www.safe182.go.kr/home/lcm/blobImgListView.do?tknphotoFileIdx={idx}"

            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            img_response = await self._download_with_retry(photo_url)
            if not img_response:
                print(f"  [{idx}] âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                break

            img_data = img_response.content
            img_size = len(img_data)

            # ë„ˆë¬´ ì‘ì€ íŒŒì¼ì€ "no image"ì¼ ê°€ëŠ¥ì„±
            if img_size < 1000:
                print(f"  [{idx}] â­ï¸  ë„ˆë¬´ ì‘ìŒ ({img_size} bytes) - ê±´ë„ˆëœ€")
                break

            # í”Œë ˆì´ìŠ¤í™€ë” í•„í„°ë§ (ì •í™•íˆ 2860 bytes)
            if img_size == 2860:
                print(f"  [{idx}] ğŸš« í”Œë ˆì´ìŠ¤í™€ë” ë°œê²¬ - ë” ì´ìƒ ì‚¬ì§„ ì—†ìŒ")
                break

            # MD5 í•´ì‹œ ê³„ì‚°
            img_hash = self._get_md5(img_data)

            # ì¤‘ë³µ ì²´í¬
            if img_hash in seen_hashes:
                print(f"  [{idx}] ğŸ” ì¤‘ë³µ ì‚¬ì§„ - ìŠ¤í‚µ")
                continue

            # ê³ ìœ í•œ ì‚¬ì§„
            seen_hashes.add(img_hash)
            photo_urls.append(photo_url)
            print(f"  [{idx}] âœ… ê³ ìœ í•œ ì‚¬ì§„! ({img_size} bytes, MD5: {img_hash[:8]}...)")

            # Rate limiting ë°©ì§€
            await asyncio.sleep(0.5)

        print(f"  ğŸ“Š ì´ {len(photo_urls)}ê°œ ì‚¬ì§„ URL ìˆ˜ì§‘ ì™„ë£Œ\n")

        # ë‹¤ìŒ ì‚¬ëŒìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ì „ ë”œë ˆì´
        if photo_urls:
            await asyncio.sleep(self.delay)

        return photo_urls

    async def scrape_multiple_persons(self, persons: List[Dict[str, str]]) -> Dict[str, List[str]]:
        """
        ì—¬ëŸ¬ ì‹¤ì¢…ìì˜ ì‚¬ì§„ ì¼ê´„ ìŠ¤í¬ë©

        Args:
            persons: [{"external_id": "...", "name": "..."}, ...]

        Returns:
            {external_id: [photo_url1, photo_url2, ...], ...}
        """
        results = {}
        total = len(persons)

        print(f"\nğŸš€ ì´ {total}ëª…ì˜ ì‚¬ì§„ ìŠ¤í¬ë© ì‹œì‘")
        print(f"â±ï¸  ìš”ì²­ ê°„ ë”œë ˆì´: {self.delay}ì´ˆ")
        print(f"ğŸ”„ ìµœëŒ€ ì¬ì‹œë„: {self.max_retries}íšŒ\n")

        for idx, person in enumerate(persons, 1):
            external_id = person.get("external_id", "")
            name = person.get("name", "Unknown")

            if not external_id:
                continue

            print(f"ì§„í–‰: {idx}/{total}")

            try:
                photo_urls = await self.scrape_person_photos(external_id, name)
                results[external_id] = photo_urls

            except Exception as e:
                print(f"  âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)[:100]}")
                results[external_id] = []
                await asyncio.sleep(self.delay)

        # í†µê³„
        total_photos = sum(len(urls) for urls in results.values())
        persons_with_photos = sum(1 for urls in results.values() if urls)

        print(f"\n{'='*80}")
        print("ğŸ“Š ìŠ¤í¬ë© ì™„ë£Œ í†µê³„")
        print(f"{'='*80}")
        print(f"  â€¢ ì²˜ë¦¬í•œ ì‹¤ì¢…ì: {total}ëª…")
        print(f"  â€¢ ì‚¬ì§„ ìˆëŠ” ì‹¤ì¢…ì: {persons_with_photos}ëª…")
        print(f"  â€¢ ì´ ìˆ˜ì§‘ ì‚¬ì§„: {total_photos}ê°œ")
        print(f"  â€¢ í‰ê·  ì‚¬ì§„/ì¸: {total_photos/total:.1f}ê°œ")
        print(f"{'='*80}\n")

        return results


async def scrape_photos_example():
    """ì‚¬ìš© ì˜ˆì œ"""
    # í…ŒìŠ¤íŠ¸ìš© ì‹¤ì¢…ì ëª©ë¡
    test_persons = [
        {"external_id": "6048080", "name": "ì´ì§„í˜„"},
        {"external_id": "6048041", "name": "ì†¡ì¸ì‹"},
    ]

    async with PhotoScraperService(delay=3.0, max_retries=3) as scraper:
        results = await scraper.scrape_multiple_persons(test_persons)

        for person_id, urls in results.items():
            print(f"{person_id}: {len(urls)}ê°œ ì‚¬ì§„")
            for url in urls:
                print(f"  - {url}")


if __name__ == "__main__":
    asyncio.run(scrape_photos_example())
